# Discussion 3/11/2025

## Key Points
- The primary focus is the estimation and inference for $\beta(t)$. For the theory part, the key result is consistency and convergence rate of $\hat{\beta}(t)$.
- Do not include penalty in the theory part. Then include penalty in the computation algorithm.
- Do not estimate $b_i(t)$ as it requires dense individual observations.
- Try multiplier bootstrap to estimate the covariance function of $\hat{\beta}(t)$. If the simulation results are good, then study the consistency of the covariance estimator.
- Find newer data on UCI.

---

## 1 Notation

- $Y(t)$: outcome process
- $X(t)$: a $d$-vector of potentially time-dependent covariates
- $N(t)$: counting process recording the number of observations
- $C$: follow-up time
- Use overline to denote the history, e.g., $\overline{N}(t) = \{N(s) : s \in [0,t]\}$
- Observed data up to time $t$:

\begin{aligned}
\overline{O}(t) = \left\{ \overline{Y}^{obs}(t \land C), \overline{N}(t \land C), \overline{X}^{obs}(t \land C) \right\},
\end{aligned}

where $x \land y = \min(x,y)$, and

\begin{aligned}
\overline{Y}^{obs}(t) = \{Y(s) : dN(s) = 1, s \in [0,t]\},
\end{aligned}

\begin{aligned}
\overline{X}^{obs}(t) = \{X(s) : dN(s) = 1, s \in [0,t]\}.
\end{aligned}

- $\mathcal{F}_t$: filtration generated by $\{\overline{Y}(t), \overline{N}(t-), \overline{X}(t)\}$

Consider a random sample of $n$ subjects, we use subscript $i$ to denote the above variables of the $i$ th subject $(i = 1, \ldots, n)$.

---

## 2 Models

### Varying coefficient model for the outcome process $Y(t)$:

\begin{aligned}
Y(t) = \beta(t)^T X(t) + b(t) + \epsilon(t), \tag{1}
\end{aligned}

where:
- $\beta(t)$ is a vector of unknown functions of $t$
- $b(t)$ is a latent stochastic process accounting for the within-curve dependence
- $\epsilon(t)$ is the measurement error

We assume that:
- $X(t)$ contains the constant 1
- $b(t)$ has mean zero and covariance function $\xi(s,t)$
- $\epsilon(t)$ has mean zero and covariance function $\sigma^2(t)I(s = t)$
- $b(t)$ and $\epsilon(t)$ are independent

- Proportional intensity model for the conditional intensity function of $N(t)$ given $\overline{O}(t-)$:

\begin{aligned}
\lambda\{t|\overline{O}(t-) \} = \lambda_0(t) \exp\left[\gamma^T g(\overline{O}(t-))\right], \tag{2}
\end{aligned}

where:
- $\lambda_0(t)$ is an arbitrary baseline intensity function
- $\gamma$ is a set of unknown parameters
- $g(\cdot)$ is a set of prespecified functions of $\overline{O}(t-)$

---

## 3 Assumptions

(i) $E\{dN(t)|F_t\} = E\{dN(t)|\overline{O}(t-)\}$.

(ii) The conditional density of $C$ at time $t$ given $\{Y(\cdot), X(\cdot), b(\cdot)\}$ depends only on $\overline{X}(t)$ and is noninformative about the unknown parameters in models (1) and (2).

---

## 4 Methods

### 4.1 Inverse intensity weighting and identifiability

By Assumption (i), we have

\begin{aligned}
\lambda\{t|\overline{O}(t-) \}^{-1}E\{Y(t)dN(t)\} = \lambda\{t|\overline{O}(t-) \}^{-1}E[E\{Y(t)dN(t)|F_t\}] = E\{Y(t)\}dt,
\end{aligned}

and for $s < t$,

\begin{aligned}
\begin{aligned}
&\lambda\{s|\overline{O}(s-) \}^{-1}\lambda\{t|\overline{O}(t-) \}^{-1} \text{Cov}\{Y(s)dN(s), Y(t)dN(t)\} \\
&= \lambda\{s|\overline{O}(s-) \}^{-1}\lambda\{t|\overline{O}(t-) \}^{-1}[E\{Y(s)dN(s)Y(t)dN(t)\} - E\{Y(s)dN(s)\}E\{Y(t)dN(t)\}] \\
&= \lambda\{s|\overline{O}(s-) \}^{-1}\lambda\{t|\overline{O}(t-) \}^{-1}E[E\{Y(s)dN(s)Y(t)dN(t)|F_t\}] - E\{Y(s)\}E\{Y(t)\}dsdt \\
&= \lambda\{s|\overline{O}(s-) \}^{-1}E\{Y(s)dN(s)Y(t)\}dt - E\{Y(s)\}E\{Y(t)\}dsdt \\
&= \lambda\{s|\overline{O}(s-) \}^{-1}E[E\{Y(s)dN(s)Y(t)|Y(t), F_s\}] - E\{Y(s)\}E\{Y(t)\}dsdt \\
&= E\{Y(s)Y(t)\}dsdt - E\{Y(s)\}E\{Y(t)\}dsdt \quad \text{[requires  dN(s)} \perp\!\!\!\perp \text{Y(t)]} \\
&= \text{Cov}\{Y(s), Y(t)\}dsdt.
\end{aligned}
\end{aligned}

Thus, weighting by $\lambda\{t|\overline{O}(t-) \}^{-1}$ creates a pseudo-population in which the observation times are no longer associated with the outcome process $Y(t)$ and valid inference can be made as if all the observed outcomes were sampled at random. Hence, $\beta(t)$ is identifiable. It follows by similar arguments that given $\beta(t)$, $b(t)$ is also identifiable.

### 4.2 Estimation of weights

We first estimate $\gamma$ by maximizing the log partial likelihood:

\begin{aligned}
\widehat{\gamma} = \arg \max_{\gamma} \sum_{i=1}^{n} \int_{0}^{C_i} \left[ \gamma^T g(\overline{O}(t-)) - \log S^{(0)}(t;\gamma) \right] dN_i(t),
\end{aligned}

where $S^{(0)}(t;\boldsymbol{\gamma})=\sum_{i=1}^{n}I(C_{i}\geq t)\exp[\boldsymbol{\gamma}^{\mathrm{T}}\boldsymbol{g}\{\overline{\mathcal{O}}(t-)]$.

Define $\Lambda_{0}(t)=\int_{0}^{t}\lambda_{0}(s)ds$. We can then obtain the Breslow estimator for $\Lambda_{0}$:

\begin{aligned}
\widehat{\Lambda}_{0}(t)=\int_{0}^{t}\frac{\sum_{i=1}^{n}I(C_{i}\geq u)dN_{i}(u)}{S^{(0)}(u;\widehat{\boldsymbol{\gamma}})}.
\end{aligned}

Finally, we obtain a kernel-smoothed estimator of $\lambda_{0}$ by

\begin{aligned}
\widehat{\lambda}_{0}(t)=h_{1n}^{-1}\int_{0}^{\tau}K\left(\frac{t-s}{h_{1n}}\right)d\widehat{\Lambda}_{0}(s),
\end{aligned}

where:
- $\tau$ is the upper bound of the support of $C_{i}$
- $K(\cdot)$ is the kernel function
- $h_{1n}$ is the bandwidth of order $n^{-1/5}$ as suggested by [1]

Let:
- $m_{i}$: number of observations for the $i$ th subject
- $t_{ij}$: $j$ th observation time of the $i$ th subject
- $(Y_{ij},\boldsymbol{X}_{ij})$: observed outcome and covariates at $t_{ij}$ ($i=1,\ldots,n$; $j=1,\ldots,m_{i}$)

From the conclusion in Section 4.1, the weight assigned to the $j$ th observation of the $i$ th subject is

\begin{aligned}
w_{ij}=\widehat{\lambda}_{0}(t_{ij})^{-1}\exp\left[-\widehat{\boldsymbol{\gamma}}^{\mathrm{T}}\boldsymbol{g}\{\overline{\mathcal{O}}(t_{ij}-)\}\right].
\end{aligned}

### 4.3 Estimation of varying coefficient functions

We approximate each component of $\boldsymbol{\beta}(t)$ by a sieve of B-spline functions:

\begin{aligned}
\boldsymbol{\beta}(t)=\boldsymbol{A}^{\mathrm{T}}\boldsymbol{B}(t),
\end{aligned}

where:
- $\boldsymbol{B}(t)$ is a $q_{n}$-vector of spline basis functions with order $z$ and $q_{n}-z$ internal knots that equally partition $[0,\tau]$
- $\boldsymbol{A}$ is a $q_{n}\times d$ matrix of spline parameters

We estimate the matrix $\boldsymbol{A}$ by minimizing the following penalized weighted least squares function:

\begin{aligned}
\widehat{\boldsymbol{A}}=\operatorname*{arg\,min}_{\boldsymbol{A}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}\left\{Y_{ij}-\boldsymbol{B}_{ij}^{\mathrm{T}}\boldsymbol{A}\boldsymbol{X}_{ij}\right\}^{2}+p(\boldsymbol{A};\boldsymbol{\eta}),
\end{aligned}

where:
- $\boldsymbol{B}_{ij}=\boldsymbol{B}(t_{ij})$
- $p(\boldsymbol{A};\boldsymbol{\eta})$ is the penalty function controlling the roughness of the estimator for $\boldsymbol{\beta}(t)$
- $\boldsymbol{\eta}=(\eta_{1},\ldots,\eta_{d})^{\mathrm{T}}$ is a $d$-vector of positive tuning parameters

One common choice for the penalty function is

\begin{aligned}
p(\boldsymbol{A};\boldsymbol{\eta})=\frac{1}{2}\mathrm{vec}(\boldsymbol{A})^{\mathrm{T}}\boldsymbol{S}_{\boldsymbol{\eta}}\mathrm{vec}(\boldsymbol{A}),
\end{aligned}

where

\begin{aligned}
\boldsymbol{S}_{\boldsymbol{\eta}}=\mathrm{diag}(\eta_{1},\ldots,\eta_{d})\otimes\int_{0}^{\tau}\left\{\boldsymbol{B}^{(k)}(t)\right\}^{\otimes 2}dt,
\end{aligned}

with $\boldsymbol{B}^{(k)}(t)$ denoting the (componentwise) $k$th order derivative of $\boldsymbol{B}(t)$, for some positive integer $k<z$.

By simple calculation, the estimator for $\mathrm{vec}(\boldsymbol{A})$ under this penalty is

\begin{aligned}
\mathrm{vec}(\widehat{\boldsymbol{A}})=\left\{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}2w_{ij}\left(\boldsymbol{X}_{ij}\otimes\boldsymbol{B}_{ij}\right)^{\otimes 2}+\boldsymbol{S}_{\boldsymbol{\eta}}\right\}^{-1}\left\{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}2w_{ij}\left(\boldsymbol{X}_{ij}\otimes\boldsymbol{B}_{ij}\right)Y_{ij}\right\}.
\end{aligned}

The estimator for $\boldsymbol{\beta}(t)$ is $\widehat{\boldsymbol{\beta}}(t)=\widehat{\boldsymbol{A}}^{\mathrm{T}}\boldsymbol{B}(t)$.

**Practical considerations:**
- Internal knots can be placed at equal quantiles of all the observation times $t_{ij}$ ($i=1,\ldots,n$ and $j=1,\ldots,m_{i}$)
- The number of internal knots and the order of spline basis functions can be determined based on leave-one-curve-out cross-validation

### 4.4 Inference on varying coefficient functions

We estimate the covariance function of $\hat{\beta}(t)$ using multiplier bootstrap [2]. Specifically, for the $l$ th bootstrap iteration:

1. Generate random multipliers $\{k_{ij}\}_{i=1,\ldots,n} \overset{iid}{\sim} \text{Exp}(1)$
2. Compute the estimator $\hat{\beta}_l(t) = \hat{A}_l^T B(t)$, where

\begin{aligned}
\hat{A}_l = \arg\min_A \sum_{i=1}^n k_{li} \sum_{j=1}^{m_j} w_{ij} \left\{ Y_{ij} - B(t_{ij})^T AX_{ij} \right\}^2 + p(A; \eta).
\end{aligned}

We repeat the above procedure $L$ times and compute the bootstrap covariance function estimator by

\begin{aligned}
\widehat{\text{Cov}} \left\{ \hat{\beta}(s), \hat{\beta}(t) \right\} = \frac{1}{L-1} \sum_{l=1}^L \left\{ \hat{\beta}_l(s) - \overline{\beta}(s) \right\} \left\{ \hat{\beta}_l(t) - \overline{\beta}(t) \right\}^T,
\end{aligned}

where $\overline{\beta}(t) = L^{-1} \sum_{l=1}^L \hat{\beta}_l(t)$.

---

## 5 Asymptotic properties

- **Estimators of varying coefficient functions:**
  - Bias of $\hat{\beta}(t)$
  - $\sqrt{n} (\hat{\beta}(t) - \text{bias}(\hat{\beta}(t)) - \beta(t))$ converges weakly to a mean-zero Gaussian process
  - Limiting covariance function of $\hat{\beta}(t)$
  - Consistency of the covariance estimator of $\hat{\beta}(t)$ via multiplier bootstrap

- **Estimators of individual functions:** bias of $\hat{b}_i(t)$

- **Estimator of covariance function of $b_i(t)$:** bias of $\hat{\xi}(s,t)$

---

## References

[1] Andersen, P. K., Borgan, O., Gill, R. D., & Keiding, N. (1993). *Statistical Models Based on Counting Processes*. New York: Springer.

[2] van der Vaart, A. W. & Wellner, J. A. (1996). *Weak Convergence and Empirical Processes*. New York: Springer.
